{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ddfed3e-bf2b-4c63-b4d5-81d889e69288",
   "metadata": {},
   "source": [
    "_This notebook is part of the material for the [Active Training Course \"Advanced Deep Learning\"](https://indico.desy.de/event/37478/)_\n",
    "\n",
    "## Generating LHC data with Transformers\n",
    "\n",
    "#### Background\n",
    "\n",
    "Simulations for LHC physics is a major computational task at the experiment, and will become more computationally intensive in the coming years, so if ML techniques can speed up some aspects of this then it would be very useful. The process we are studying is Drell-Yan: $pp\\rightarrow Z\\rightarrow \\mu\\mu$\n",
    "\n",
    "#### Interesting papers\n",
    "\n",
    "- Jet Diffusion versus JetGPT â€” Modern Networks for the LHC<br>\n",
    "  *Anja Butter, Nathan Huetsch, Sofia Palacios Schweitzer, Tilman Plehn, Peter Sorrenson, and Jonas Spinner*<br>\n",
    "  https://arxiv.org/pdf/2305.10475.pdf\n",
    "- Denoising Diffusion Probabilistic Models<br>\n",
    "  *Jonathan Ho, Ajay Jain, Pieter Abbeel*<br>\n",
    "  https://arxiv.org/pdf/2006.11239.pdf\n",
    "\n",
    "#### Outline\n",
    "- Imports\n",
    "- Toy example for attention\n",
    "- Loading the data and preprocessing\n",
    "- Building a generative transformer step by step\n",
    "- Training the model\n",
    "- Generating samples\n",
    "- Study the results\n",
    "- Bonus exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c988ee6c-46ef-4b4b-92b4-eaf3952a7520",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627c0349-07f8-476b-84ba-5e61afe7a672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as D\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "# datapath when not using COLAB\n",
    "path = \"datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a982afcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device(\"cuda\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a9259",
   "metadata": {},
   "source": [
    "### Toy example for attention\n",
    "\n",
    "Before we start building a generative transformer for the LHC data from the last tutorial, we will take a look at the attention mechanism itself for a toy example. This part of the tutorial is based on Lukas Heinrich's example transformer from the MITP school 2023 \"Machine Learning in Particle Theory\".\n",
    "\n",
    "#### Exercise 1: Toy attention\n",
    "\n",
    "We will use attention to build a classifier to find out whether there are more 4s than 2s in a sequence of integers from 0 to 9. This allows us to visualize the attention matrix and values and understand what the attention mechanism is doing in this simple example.\n",
    "\n",
    "Complete the attention mechanism in the `ToyAttention` class below. Hint: the function `torch.einsum` makes it easy to translate the equations from the lecture into PyTorch code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e8a708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(batch_size):\n",
    "    x = torch.randint(0,10, size=(batch_size, 10))\n",
    "    count_2 = torch.count_nonzero(x == 2, dim=-1)\n",
    "    count_4 = torch.count_nonzero(x == 4, dim=-1)\n",
    "    return x, (count_4 > count_2).reshape(-1,1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6427b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyAttention(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.query = torch.nn.Parameter(torch.randn(1,16))\n",
    "        self.embedding_func = torch.nn.Embedding(10, embedding_dim=16)\n",
    "        self.key_func = torch.nn.Linear(16,16)\n",
    "        self.value_func = torch.nn.Linear(16,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedding = self.embedding_func(x)\n",
    "        keys = self.key_func(embedding)\n",
    "        values = self.value_func(embedding)\n",
    "        \n",
    "        attention_normalized = # YOUR STUFF HERE\n",
    "        result = # YOUR STUFF HERE\n",
    "        return result, attention_normalized, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fcc142",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_attention = ToyAttention()\n",
    "optimizer = torch.optim.Adam(toy_attention.parameters(), lr=1e-2)\n",
    "loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "for batches in range(5000):\n",
    "    x, y = generate_data(1024)\n",
    "    result, attention, values = toy_attention(x)\n",
    "    loss = loss_func(result, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if batches % 100 == 0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce927cf",
   "metadata": {},
   "source": [
    "Now let's test it for some example sequence to understand what the attention is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f8da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.tensor([[4,3,5,7,8,4,2,4,3,0]])\n",
    "    result, attention, values = toy_attention(x)\n",
    "print(\"Result:\", torch.sigmoid(result).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a9fab6",
   "metadata": {},
   "source": [
    "First we can visualize the attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3bbaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,1))\n",
    "plt.imshow(attention[0])\n",
    "plt.xticks(np.arange(10), x[0].numpy())\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"test sequence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920d4e88",
   "metadata": {},
   "source": [
    "Next we can take a look at the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a68f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,1))\n",
    "vals = values[0,:,:].T\n",
    "vals[attention[0] < 0.1] = np.nan\n",
    "plt.imshow(vals)\n",
    "plt.xticks(np.arange(10), x[0].cpu().numpy())\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"test sequence\")\n",
    "for i, v in enumerate(vals[0].cpu().numpy()):\n",
    "    plt.text(i - 0.25, 0., \"\" if np.isnan(v) else f\"{v:.2f}\", c=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839131f4",
   "metadata": {},
   "source": [
    "### Loading the data and preprocessing\n",
    "\n",
    "We will use the same preprocessing as in the last tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d7d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY when using Colab, execute the following lines\n",
    "!wget -O ErUMData.zip https://www.dropbox.com/scl/fi/gvmelw7u619moo8nyg3j7/ErUMData.zip?rlkey=kq4do1fmalppjt2v24lzau4li&dl=1\n",
    "!unzip -q ErUMData.zip \n",
    "!rm ErUMData.zip\n",
    "path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a81aa7-d345-45a0-921a-bf4c78fead61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(f\"{path}dy_trn_data.npy\")\n",
    "val_data = np.load(f\"{path}dy_val_data.npy\")\n",
    "test_data = np.load(f\"{path}dy_tst_data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a237c8e2-d522-4a7d-a621-fb6854a2e541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_preprocessing(data_full):\n",
    "    pt = np.sqrt(data_full[:,1]**2 + data_full[:,2]**2)\n",
    "    eta1 = np.arctanh(data_full[:,3] / np.sqrt(data_full[:,1]**2 + data_full[:,2]**2 + data_full[:,3]**2))\n",
    "    eta2 = np.arctanh(data_full[:,7] / np.sqrt(data_full[:,5]**2 + data_full[:,6]**2 + data_full[:,7]**2))\n",
    "    phi1 = np.arctanh(np.arctan2(data_full[:,2], data_full[:,1]) / np.pi)\n",
    "    return np.stack((pt, eta1, eta2, phi1), axis=1)\n",
    "    \n",
    "def invert_preprocessing(data_red):\n",
    "    pt = data_red[...,0]\n",
    "    eta1 = data_red[...,1]\n",
    "    eta2 = data_red[...,2]\n",
    "    phi1 = np.arctan(data_red[...,3]) * np.pi\n",
    "    px1 = pt * np.cos(phi1)\n",
    "    py1 = pt * np.sin(phi1)\n",
    "    pz1 = pt * np.sinh(eta1)\n",
    "    e1 = np.sqrt(px1**2 + py1**2 + pz1**2)\n",
    "    px2 = -px1\n",
    "    py2 = -py1\n",
    "    pz2 = pt * np.sinh(eta2)\n",
    "    e2 = np.sqrt(px2**2 + py2**2 + pz2**2)\n",
    "    return np.stack((e1, px1, py1, pz1, e2, px2, py2, pz2), axis=-1)\n",
    "\n",
    "def get_obs(event):\n",
    "    jet1_4m = event[:,0:4]\n",
    "    jet2_4m = event[:,4:]\n",
    "    event_4m = jet1_4m + jet2_4m\n",
    "    event_im = np.sqrt(event_4m[:,0]**2 - event_4m[:,1]**2 - event_4m[:,2]**2 - event_4m[:,3]**2)\n",
    "    pt = np.sqrt(jet1_4m[:,1]**2 + jet1_4m[:,2]**2)\n",
    "    return event_im, pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3bae77-5f62-4cde-8388-b16bd6062357",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_preproc = apply_preprocessing(train_data)\n",
    "train_mean = np.mean(train_data_preproc, axis=0)\n",
    "train_std = np.std(train_data_preproc, axis=0)\n",
    "train_data_normalized = torch.tensor((train_data_preproc - train_mean) / train_std, dtype=torch.float32)\n",
    "train_dataset = TensorDataset(train_data_normalized)\n",
    "train_dataloader = DataLoader(train_data_normalized, batch_size=1024, shuffle=True, generator=torch.Generator(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e979dc-4dab-4597-935b-977ce0ac1dcf",
   "metadata": {},
   "source": [
    "### Building a generative transformer step by step\n",
    "\n",
    "In the following exercises, we will build a generative transformer for the Drell-Yan data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164ac579",
   "metadata": {},
   "source": [
    "#### Exercise 2: Build the attention mask\n",
    "\n",
    "To make the transformer autoregressive, we have to make sure that every generated feature is only dependent on the features before, but not itself or the following features. This can be achieved by setting some of the elements of the attention matrix to zero. We can pass a boolean tensor to the torch.TransformerEncoder to do this masking. Construct such a mask and put it in the function `attention_mask` function of the `JetGPT` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5784b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check here if your matrix looks correct before you put it into the JetGPT class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdee9bd0",
   "metadata": {},
   "source": [
    "#### Exercise 3: Build the Gaussian mixture model\n",
    "\n",
    "Finally, we need to define some distribution to calculate the likelihood of the training data. In contrast to language processing, we are not dealing with a discrete distribution. So we need some continuous distribution that is expressive enough to model the distribution of the data. There are different options to do this. In this example, we will use a Gaussian mixture model.\n",
    "\n",
    "Pytorch provides the module `torch.distributions`. We can use it to construct a `Distribution` object that has functions to compute the log-likelihood and draw samples. Complete the function `gaussian_mixture_model` in the `JetGPT` class. The function first splits its argument into the means, standard deviations and relative magnitudes of the Gaussians. We use an exponential to ensure positive standard deviations and a softmax to ensure properly normalized magnitudes.\n",
    "\n",
    "Hint: Use the classes `D.Categorical`, `D.Normal`, `D.MixtureSameFamily`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f986330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot of samples from your GMM here to see if it looks correct before you put it into the JetGPT class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdba9c24",
   "metadata": {},
   "source": [
    "#### Exercise 4: Build the embedding\n",
    "\n",
    "The transformer can't operate on the training data directly. Instead, we have to embed it into the transformer embedding space. This space has to contain the information about the training feature itself and about its position, since the transformer by itself is permutation invariant. There are several options to build this embedding:\n",
    "- Concatenate the feature and a one-hot encoded representation of the position. Then use a linear layer to map it into the embedding space.\n",
    "- Map the feature into the embedding space using a linear layer and use torch.nn.Encoding for the position. Then add the two resulting vectors.\n",
    "Choose one of these options (both should work fine) and implement it in the `embedding` function of the `JetGPT` class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ad0bb6-c59c-46db-b616-65a09b0fb15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JetGPT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dim: int,           # number of features in the data\n",
    "        embedding_dim: int,      # number of features in the transformer\n",
    "        heads: int,              # number of attention heads\n",
    "        feedforward_dim: int,    # dimension of the feed-forward layers in the transformer\n",
    "        transformer_layers: int, # number of transformer layers\n",
    "        mlp_layers: int,         # number of layers in the final MLP\n",
    "        mlp_hidden_dim: int,     # number of hidden nodes in the final MLP\n",
    "        gaussians: int,          # number of Gaussians in the Gaussian mixture model\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dim = data_dim\n",
    "        \n",
    "        # INITIALIZE STUFF FOR EMBEDDING HERE\n",
    "\n",
    "        # Build transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=heads,\n",
    "            dim_feedforward=feedforward_dim,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer=encoder_layer, num_layers=transformer_layers)\n",
    "\n",
    "        # Build final MLP\n",
    "        layers = []\n",
    "        layer_dim_in = embedding_dim\n",
    "        for i in range(mlp_layers - 1):\n",
    "            layers.append(nn.Linear(layer_dim_in, mlp_hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layer_dim_in = mlp_hidden_dim\n",
    "        layers.append(nn.Linear(layer_dim_in, gaussians * 3))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def embedding(\n",
    "        self,\n",
    "        x: torch.Tensor, # input data, shape (n_batch, seq_len)\n",
    "    ) -> torch.Tensor:   # returns data in embedding space, shape (n_batch, seq_len [+1], data_dim + 1)\n",
    "        shifted_x = torch.cat((torch.zeros((x.shape[0], 1)), x), dim=1)\n",
    "        \n",
    "        # BUILD YOUR EMBEDDING HERE\n",
    "        return \n",
    "    \n",
    "    def attention_mask(\n",
    "        self,\n",
    "        size: int      # Size of the attention mask\n",
    "    ) -> torch.Tensor: # Boolean tensor, shape (size, size)\n",
    "        # BUILD YOUR ATTENTION MASK HERE\n",
    "        return\n",
    "\n",
    "    def eval_networks(\n",
    "        self,\n",
    "        embedding: torch.Tensor, # data in embedding space, shape (n_batch, seq_len [+1], data_dim + 1)\n",
    "    ) -> torch.Tensor:           # parameters of the GMM, shape (n_batch, seq_len [+1], 3 * gaussians)\n",
    "        transformer_output = self.transformer(embedding, self.attention_mask(embedding.shape[1]))\n",
    "        return self.mlp(transformer_output)\n",
    "\n",
    "    def gaussian_mixture_model(\n",
    "        self,\n",
    "        gmm_params: torch.Tensor, # parameters of the GMM, shape (n_batch, seq_len [+1], 3 * gaussians)\n",
    "    ) -> D.Distribution:          # Distribution object encoding the GMM\n",
    "        mu = gmm_params[:, :, 0 :: 3]\n",
    "        sigma = torch.exp(gmm_params[:, :, 1 :: 3])\n",
    "        weights = F.softmax(gmm_params[:, :, 2 :: 3], dim=2)\n",
    "        \n",
    "        # BUILD YOUR GAUSSIAN MIXTURE MODEL HERE\n",
    "        return\n",
    "\n",
    "    def input_to_gmm(\n",
    "        self,\n",
    "        x: torch.Tensor, # input data, shape (n_batch, seq_len)\n",
    "    ) -> D.Distribution: # Distribution object encoding the GMM\n",
    "        embedding = self.embedding(x)\n",
    "        gmm_params = self.eval_networks(embedding)\n",
    "        return self.gaussian_mixture_model(gmm_params)\n",
    "\n",
    "    def log_prob(\n",
    "        self,\n",
    "        x: torch.Tensor, # input data, shape (n_batch, data_dim)\n",
    "    ) -> torch.Tensor:   # log-probabilities, shape (n_batch, )\n",
    "        return self.input_to_gmm(x[:, :-1]).log_prob(x).sum(dim=1)\n",
    "\n",
    "    def sample(\n",
    "        self,\n",
    "        n_samples: int, # number of samples\n",
    "    ) -> torch.Tensor:  # sampled data, shape (n_samples, data_dim)\n",
    "        x = torch.zeros((n_samples, 0))\n",
    "        for i in range(self.data_dim):\n",
    "            # COMPLETE THE SAMPLE FUNCTION HERE\n",
    "            x = torch.cat((x, x_new), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ea527a-cd5d-4188-9b94-2d6cf622280e",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "After solving exercises 2 to 4, you should be able to train your transformer network with the training loop provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e1458-b1f0-428d-8b6f-8e6448713aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_gpt = JetGPT(\n",
    "    data_dim = 4,\n",
    "    embedding_dim = 64,\n",
    "    heads = 4,\n",
    "    feedforward_dim = 128,\n",
    "    transformer_layers = 4,\n",
    "    mlp_layers = 3,\n",
    "    mlp_hidden_dim = 64,\n",
    "    gaussians = 10,\n",
    ")\n",
    "optimizer = torch.optim.Adam(jet_gpt.parameters(), lr=3e-4) # https://twitter.com/karpathy/status/801621764144971776\n",
    "\n",
    "for epoch in range(5):\n",
    "    epoch_losses = []\n",
    "    for batch, x in enumerate(train_dataloader):\n",
    "        loss = -jet_gpt.log_prob(x).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "    print(f\"Epoch {epoch+1}: loss = {np.mean(epoch_losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d31c4a-6534-4b1c-809d-3b9fecea2a0b",
   "metadata": {},
   "source": [
    "### Generating samples\n",
    "\n",
    "Now that we have a trained transformer, the last step is to draw samples from the learned distribution.\n",
    "\n",
    "#### Exercise 5: Generating samples\n",
    "\n",
    "Complete the `sample` function of the `JetGPT` class. This time you need to use the sample function of the `Distribution` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe979a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sample = jet_gpt.sample(100000).cpu().numpy()\n",
    "sample_pp = invert_preprocessing(sample * train_std + train_mean)\n",
    "gen_event_im, gen_pt = get_obs(sample_pp)\n",
    "test_event_im, test_pt = get_obs(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a49bfd8",
   "metadata": {},
   "source": [
    "### Study the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa298cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "bins = np.linspace(0,200,50)\n",
    "axs.hist(sample_pp[:,0], bins=bins, density=True, histtype=\"step\", label=\"generated\")\n",
    "axs.hist(test_data[:,0], bins=bins, density=True, histtype=\"step\", label=\"truth\")\n",
    "\n",
    "axs.set_xlabel(\"$E_1$ (GeV)\")\n",
    "axs.set_ylabel(\"normalized\")\n",
    "axs.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70034ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "bins = np.linspace(0, 70, 50)\n",
    "axs.hist(gen_pt, bins=bins, density=True, histtype=\"step\", label=\"generated\")\n",
    "axs.hist(test_pt, bins=bins, density=True, histtype=\"step\", label=\"truth\")\n",
    "\n",
    "axs.set_xlabel(\"$p_T$ (GeV)\")\n",
    "axs.set_ylabel(\"normalized\")\n",
    "axs.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6f71ae-4a5f-4062-820c-039dbc3256c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "bins = np.linspace(60, 120, 50)\n",
    "axs.hist(gen_event_im, bins=bins, density=True, histtype=\"step\", label=\"generated\")\n",
    "axs.hist(test_event_im, bins=bins, density=True, histtype=\"step\", label=\"truth\")\n",
    "\n",
    "axs.set_xlabel(\"$m_{\\mu\\mu}$ (GeV)\")\n",
    "axs.set_ylabel(\"normalized\")\n",
    "axs.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72275c14",
   "metadata": {},
   "source": [
    "### Bonus exercises\n",
    "\n",
    "#### Exercise A\n",
    "\n",
    "Make plots for some more observables, for example $\\phi$ or $\\eta$. Which ones are easy for the network and which ones are more difficult?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a011c1a",
   "metadata": {},
   "source": [
    "#### Exercise B\n",
    "\n",
    "Play around with the hyperparameters of the network. Which ones have a large impact on the performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e65b48b",
   "metadata": {},
   "source": [
    "#### Exercise C\n",
    "\n",
    "Since our generative transformer is autoregressive, it makes a difference which order we choose for the input features. Try a different ordering and see if it makes a difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068ef24a",
   "metadata": {},
   "source": [
    "#### Exercise D\n",
    "\n",
    "We can even randomize the order of the components during the training and train the network for arbitrary orders. The positional encoding ensures that every component is in the right place in the end. Implement that for our JetGPT training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370dfed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
