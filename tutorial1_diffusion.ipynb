{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "737ee93d-c6fe-40d5-8430-ccc54e2b806b",
   "metadata": {},
   "source": [
    "_This notebook is part of the material for the [Active Training Course \"Advanced Deep Learning\"](https://indico.desy.de/event/37478/)_\n",
    "\n",
    "## Generating LHC data with Diffusion Models\n",
    "\n",
    "#### Background\n",
    "\n",
    "Simulations for LHC physics is a major computational task at the experiment, and will become more computationally intensive in the coming years, so if ML techniques can speed up some aspects of this then it would be very useful. The process we are studying is Drell-Yan: $pp\\rightarrow Z\\rightarrow \\mu\\mu$\n",
    "\n",
    "#### Interesting papers\n",
    "\n",
    "- Jet Diffusion versus JetGPT â€” Modern Networks for the LHC<br>\n",
    "  *Anja Butter, Nathan Huetsch, Sofia Palacios Schweitzer, Tilman Plehn, Peter Sorrenson, and Jonas Spinner*<br>\n",
    "  https://arxiv.org/pdf/2305.10475.pdf\n",
    "- Attention Is All You Need<br>\n",
    "  *Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin*<br>\n",
    "  https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "#### Outline\n",
    "\n",
    "- Imports\n",
    "- Loading the data\n",
    "- Study the data\n",
    "- Preprocessing\n",
    "- Defining the diffusion model\n",
    "- Training the model\n",
    "- Study the results\n",
    "- Visualizing the diffusion process\n",
    "- Bonus exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e0b677-dc37-4393-a2c9-39ae6034ac9b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7f0a8e-d95b-4a62-8e6b-fcd9c535c1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "# datapath when not using COLAB\n",
    "path = \"datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8de9a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device(\"cuda\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc98abe-dd85-4160-a885-1933cd2a5fb5",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506edd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY when using Colab, execute the following lines\n",
    "!wget -O ErUMData.zip https://www.dropbox.com/scl/fi/gvmelw7u619moo8nyg3j7/ErUMData.zip?rlkey=kq4do1fmalppjt2v24lzau4li&dl=1\n",
    "!unzip -q ErUMData.zip \n",
    "!rm ErUMData.zip\n",
    "path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34b77ab-3f40-4925-af92-d3e692ff67ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(f\"{path}dy_trn_data.npy\")\n",
    "val_data = np.load(f\"{path}dy_val_data.npy\")\n",
    "test_data = np.load(f\"{path}dy_tst_data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb355dd-752f-4e2f-b45c-45c7cdcd48bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape, val_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7614e7f4-117a-48cd-885b-856703e521e1",
   "metadata": {},
   "source": [
    "Each element of the data has 8 entries, corresponding to the 4-momenta of each muon in the process.\n",
    "\n",
    "Each entry has the form $[E_1, p_{x,1}, p_{y,1}, p_{z,1}, E_2, p_{x,2}, p_{y,2}, p_{z,2}]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a49ce6-62b8-4431-ab7f-2db9a42e0351",
   "metadata": {},
   "source": [
    "### Study the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dda6ba-1d9d-471e-9e3a-77228c83c422",
   "metadata": {},
   "source": [
    "We need to be able to calculate the invariant mass of each event, and the $p_T$ of the event and the jets.\n",
    "\n",
    "Let's write a function for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fbdefc-fdad-4861-8b17-0527f5fe2874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obs(events):\n",
    "    muon1_4m = events[...,0:4]\n",
    "    muon2_4m = events[...,4:]\n",
    "    event_4m = muon1_4m + muon2_4m\n",
    "    \n",
    "    event_im = np.sqrt(event_4m[...,0]**2 - event_4m[...,1]**2 - event_4m[...,2]**2 - event_4m[...,3]**2)\n",
    "    \n",
    "    event_pt = np.sqrt(event_4m[...,1]**2 + event_4m[...,2]**2)\n",
    "    muon1_pt = np.sqrt(muon1_4m[...,1]**2 + muon1_4m[...,2]**2)\n",
    "    muon2_pt = np.sqrt(muon2_4m[...,1]**2 + muon2_4m[...,2]**2)\n",
    "    \n",
    "    return event_im, event_pt, muon1_pt, muon2_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049687a0-8ff9-4d82-938c-b91859048f4d",
   "metadata": {},
   "source": [
    "Now test it for the first two events in the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0261228-e529-415d-a97a-7edc0aec986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_obs(train_data[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52e0881-033b-4d45-bb17-9a9093a73eb6",
   "metadata": {},
   "source": [
    "The $p_T$ of the events will always be zero here due to the fact that the initial incoming protons have no transverse momentum. For the same reason, the $p_T$ of the first and second muon are equal.\n",
    "\n",
    "Let's get the observables for the whole test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d27a11-d5cf-4cd4-9648-d56b82d31540",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_event_ims, test_event_pts, test_muon1_pts, test_muon2_pts = get_obs(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba9960c-ec58-4e84-9f27-27c31cf7d263",
   "metadata": {},
   "source": [
    "Let's plot the invariant mass of the events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00537857-b7f4-4a8b-b1d6-21242340a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "axs.hist(test_event_ims, density=True, bins=50, histtype=\"step\")\n",
    "axs.set_xlabel(\"$m_{\\mu\\mu}$ (GeV)\")\n",
    "axs.set_ylabel(\"normalized\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474bde4d-3a29-40af-a0fe-43648590e45b",
   "metadata": {},
   "source": [
    "Here we see the clear $Z$ mass peak at $\\simeq 90$ GeV. Now let's look at $p_T$ distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa3b1fa-1149-43d8-8f19-d6dc458931a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "bins = np.linspace(0, 70, 50)\n",
    "axs.hist(test_muon1_pts, density=True, bins=bins, histtype=\"step\", label=r\"$\\mu_1$\")\n",
    "axs.hist(test_muon2_pts, density=True, bins=bins, histtype=\"step\", label=r\"$\\mu_2$\")\n",
    "\n",
    "axs.set_xlabel(\"$p_{T}$ (GeV)\")\n",
    "axs.set_ylabel(\"normalized\")\n",
    "axs.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87150e96-e85d-45c2-b4fe-a7e45b305132",
   "metadata": {},
   "source": [
    "These distributions are completely overlapping with a peak near $m_Z/2$.\n",
    "\n",
    "In the collider the incoming particles have no momentum in the $x$ or $y$ directions, therefore momentum conservation means that the final state should also not have any momentum in these directions.  This means that we have redundant degrees of freedom that can be removed from the data.\n",
    "\n",
    "We can check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0005a1-1dbf-4724-adb8-8f6b47c6edc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "bins = np.arange(-5, 5, step=0.1)\n",
    "axs.hist(test_data[:,1]+test_data[:,5], density=True, bins=bins, histtype=\"step\", label=\"$p_{x,1}+p_{x_2}$\")\n",
    "axs.hist(test_data[:,2]+test_data[:,6], density=True, bins=bins, histtype=\"step\", label=\"$p_{y,1}+p_{y_2}$\")\n",
    "\n",
    "axs.set_xlabel(\"$p_T$ (GeV)\")\n",
    "axs.set_ylabel(\"normalized\")\n",
    "axs.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93449e0b-5be2-4382-871d-136ed4164b1c",
   "metadata": {},
   "source": [
    "Muons have a mass of about $106$ MeV.  This is much smaller than the typical energy scales involved in the Drell-Yan process we are studying, so we can assume to a good approximation that the mass of the final state particles are zero.  These final state particles are on-shell, i.e. $p^2=m^2=0$, so we can also use this as a constraint.  We can check that this approximately holds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6105a06a-bea3-40a9-96bf-0f849c815cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "bins = np.arange(-5, 5, step=0.1)\n",
    "axs.hist(test_data[:,0]**2-test_data[:,1]**2-test_data[:,2]**2-test_data[:,3]**2, \n",
    "         density=True, bins=bins, label=\"$m_1^2$\", histtype=\"step\")\n",
    "axs.hist(test_data[:,4]**2-test_data[:,5]**2-test_data[:,6]**2-test_data[:,7]**2, \n",
    "         density=True, bins=bins, label=\"$m_2^2$\", histtype=\"step\")\n",
    "\n",
    "axs.set_xlabel(\"$m^2$ (GeV$^2$)\")\n",
    "axs.set_ylabel(\"normalized\")\n",
    "axs.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fe176e-ebb2-4057-861c-d62f71a25386",
   "metadata": {},
   "source": [
    "We can remove 4 degrees of freedom from the data. For the remaining 4 degrees of freedom, we can choose a more suitable representation that takes into account the symmetries of our data:\n",
    "- $p_T = \\sqrt{p_{x,1}^2+p_{y,1}^2}$\n",
    "- $\\eta_1 = \\text{arctanh}\\left(\\frac{p_{z,1}}{\\sqrt{p_{x,1}^2+p_{y,1}^2+p_{z,1}^2}}\\right)$\n",
    "- $\\eta_2 = \\text{arctanh}\\left(\\frac{p_{z,2}}{\\sqrt{p_{x,2}^2+p_{y,2}^2+p_{z,2}^2}}\\right)$\n",
    "- $\\phi = \\text{arctan2}(p_{y,1}, p_{x,1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76421d03-2900-434f-96c6-24b8472c8d86",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18785a59-a27e-46e6-99d2-8d2891d1a4a6",
   "metadata": {},
   "source": [
    "For data to work well with generative models operating on a Gaussian latent space (a typical choice for diffusion models or normalizing flows), we should avoid sharp edges in the input data. We know that $\\phi$ follows a uniform distribution, so we can transform it into something closer to a Gaussian by applying $\\text{arctanh}$. We also apply a component-wise normalization step such that the data has a mean of $0$ and a standard deviation of $1$.\n",
    "\n",
    "Let's write some functions to apply the preprocessing to the training data and invert it for the generated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6fbae4-c011-41d0-9203-5938dce37ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_preprocessing(data_full):\n",
    "    pt = np.sqrt(data_full[...,1]**2 + data_full[...,2]**2)\n",
    "    eta1 = np.arctanh(data_full[...,3] / np.sqrt(data_full[...,1]**2 + data_full[...,2]**2 + data_full[...,3]**2))\n",
    "    eta2 = np.arctanh(data_full[...,7] / np.sqrt(data_full[...,5]**2 + data_full[...,6]**2 + data_full[...,7]**2))\n",
    "    phi1 = np.arctanh(np.arctan2(data_full[...,2], data_full[...,1]) / np.pi)\n",
    "    return np.stack((pt, eta1, eta2, phi1), axis=-1)\n",
    "    \n",
    "def invert_preprocessing(data_red):\n",
    "    pt = data_red[...,0]\n",
    "    eta1 = data_red[...,1]\n",
    "    eta2 = data_red[...,2]\n",
    "    phi1 = np.arctan(data_red[...,3]) * np.pi\n",
    "    px1 = pt * np.cos(phi1)\n",
    "    py1 = pt * np.sin(phi1)\n",
    "    pz1 = pt * np.sinh(eta1)\n",
    "    e1 = np.sqrt(px1**2 + py1**2 + pz1**2)\n",
    "    px2 = -px1\n",
    "    py2 = -py1\n",
    "    pz2 = pt * np.sinh(eta2)\n",
    "    e2 = np.sqrt(px2**2 + py2**2 + pz2**2)\n",
    "    return np.stack((e1, px1, py1, pz1, e2, px2, py2, pz2), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a07035-8562-4a91-b7b9-dacef6495a45",
   "metadata": {},
   "source": [
    "Now we can apply them to the training data and create a torch Dataset and DataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e04d4f2-91c5-4518-9c9d-2ed676a54bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_preproc = apply_preprocessing(train_data)\n",
    "train_mean = np.mean(train_data_preproc, axis=0)\n",
    "train_std = np.std(train_data_preproc, axis=0)\n",
    "train_data_normalized = torch.tensor((train_data_preproc - train_mean) / train_std, dtype=torch.float32)\n",
    "train_dataset = TensorDataset(train_data_normalized)\n",
    "train_dataloader = DataLoader(train_data_normalized, batch_size=1024, shuffle=True, generator=torch.Generator(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df723b1-97b4-4bc3-b90a-fd473c62b6c3",
   "metadata": {},
   "source": [
    "Let's take a look at the data the generative model has to learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8703707-aa97-4545-b855-5bd71fc94d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "bins = np.arange(-5, 5, step=0.1)\n",
    "for i in range(4):\n",
    "    axs.hist(\n",
    "        train_data_normalized[:, i].cpu().numpy(), density=True, bins=bins, label=f\"input {i}\", histtype=\"step\"\n",
    "    )\n",
    "\n",
    "axs.set_xlabel(\"training data\")\n",
    "axs.set_ylabel(\"normalized\")\n",
    "axs.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc979af-fad1-4f02-b3ed-268354b6062d",
   "metadata": {},
   "source": [
    "### Defining the diffusion model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638ab32f",
   "metadata": {},
   "source": [
    "#### Exercise 1: Diffusion loss function\n",
    "\n",
    "Implement the diffusion model loss function using the following steps:\n",
    "1. Randomly draw the timestep $t$.\n",
    "2. Sample Gaussian noise.\n",
    "3. Compute $x_t$ using the function `compute_xt`\n",
    "4. Compute the prefactor of the loss using `compute_relative_factor`\n",
    "5. Call the network as a function of $x_t$ and $t$\n",
    "6. Compute the MSE loss with the correct prefactor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cca89f8-ee10-4f50-bb0d-7a86d773e96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JetDiffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dim: int,     # number of features in the data\n",
    "        n_steps: int,      # number of time steps\n",
    "        n_layers: int,     # number of network layers\n",
    "        hidden_dim: int,   # number of hidden layer nodes\n",
    "        beta_schedule: str # which schedule to use for the diffusion, \"linear\" or \"cosine\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dim = data_dim\n",
    "        self.n_steps = n_steps\n",
    "\n",
    "        # Initialize alpha, beta and sigma constants for the given number of time steps\n",
    "        if beta_schedule == \"linear\":\n",
    "            self.betas = self.linear_beta_schedule(n_steps)\n",
    "        elif beta_schedule == \"cosine\":\n",
    "            self.betas = self.cosine_beta_schedule(n_steps)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown schedule\")\n",
    "        alphas = 1 - self.betas\n",
    "        alphas_bar = torch.cumprod(alphas, dim=0)\n",
    "        alphas_bar_prev = F.pad(alphas_bar[:-1], (1, 0), value=1.)\n",
    "        self.one_minus_alphas_bar = 1 - alphas_bar\n",
    "        self.sqrt_alphas = torch.sqrt(alphas)\n",
    "        self.sqrt_alphas_bar = torch.sqrt(alphas_bar)\n",
    "        self.sqrt_one_minus_alphas_bar = torch.sqrt(self.one_minus_alphas_bar)\n",
    "        self.sigmas = torch.sqrt(self.betas)\n",
    "\n",
    "        # Build network\n",
    "        layers = []\n",
    "        layer_dim_in = data_dim + 1\n",
    "        for i in range(n_layers - 1):\n",
    "            layers.append(nn.Linear(layer_dim_in, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layer_dim_in = hidden_dim\n",
    "        layers.append(nn.Linear(layer_dim_in, data_dim))\n",
    "        torch.nn.init.zeros_(layers[-1].weight)\n",
    "        torch.nn.init.zeros_(layers[-1].bias)\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def linear_beta_schedule(\n",
    "        self,\n",
    "        n_steps: int   # number of time steps\n",
    "    ) -> torch.Tensor: # beta values at the time steps, shape (n_steps, )\n",
    "        scale = 1000 / n_steps\n",
    "        beta_start = scale * 0.0001\n",
    "        beta_end = scale * 0.02\n",
    "        return torch.linspace(beta_start, beta_end, n_steps, dtype=torch.float64)\n",
    "\n",
    "    def cosine_beta_schedule(\n",
    "        self,\n",
    "        n_steps: int,    # number of time steps\n",
    "        s: float = 0.008 # offset preventing small beta near t = 0\n",
    "    ) -> torch.Tensor:   # beta values at the time steps, shape (n_steps, )\n",
    "        x = torch.linspace(0, n_steps, n_steps+1, dtype = torch.float64)\n",
    "        alphas_cumprod = torch.cos(((x / n_steps) + s) / (1 + s) * np.pi * 0.5) ** 2\n",
    "        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "        return torch.clip(betas, 0, 0.999)\n",
    "\n",
    "    def compute_xt(\n",
    "        self,\n",
    "        x0: torch.Tensor,    # data point x_0, shape (n_batch, data_dim)\n",
    "        t: torch.Tensor,     # time step, shape (n_batch, 1)\n",
    "        noise: torch.Tensor, # gaussian noise, shape (n_batch, data_dim)\n",
    "    ) -> torch.Tensor:       # noisy point x_t, shape (n_batch, data_dim)\n",
    "        return self.sqrt_alphas_bar[t] * x0 + self.sqrt_one_minus_alphas_bar[t] * noise\n",
    "\n",
    "    def compute_relative_factor(\n",
    "        self,\n",
    "        t: torch.Tensor, # time step, shape (n_batch, 1)\n",
    "    ) -> torch.Tensor:   # prefactor inside the MSE loss, shape (n_batch, 1)\n",
    "        return self.betas[t] / (np.sqrt(2) * self.sigmas[t] * self.sqrt_alphas[t] * self.sqrt_one_minus_alphas_bar[t])\n",
    "\n",
    "    def compute_mu_tilde_t(\n",
    "        self,\n",
    "        xt: torch.Tensor,    # noisy point x_t, shape (n_batch, data_dim)\n",
    "        t: torch.Tensor,     # time step, shape (n_batch, 1)\n",
    "        noise: torch.Tensor, # gaussian noise, shape (n_batch, data_dim)\n",
    "    ) -> torch.Tensor:       # computes mu_tilde_t, shape (n_batch, data_dim)\n",
    "        return (xt - noise * self.betas[t] / self.sqrt_one_minus_alphas_bar[t]) / self.sqrt_alphas[t]\n",
    "\n",
    "    def batch_loss(\n",
    "        self,\n",
    "        x: torch.Tensor, # input data, shape (n_batch, data_dim)\n",
    "    ) -> torch.Tensor:   # loss, shape (n_batch, )\n",
    "        # YOUR LOSS HERE\n",
    "        return\n",
    "\n",
    "    def sample(\n",
    "        self,\n",
    "        n_samples: int,        # number of samples\n",
    "        keep_xt: bool = False, # whether to keep the intermediate x_t (TO BE IMPLEMENTED IN EXERCISE 4)\n",
    "    ) -> torch.Tensor:         # sampled data, shape (n_samples, data_dim) or (n_steps, n_samples, data_dim)\n",
    "        x = torch.randn(n_samples, self.data_dim)\n",
    "        for t in reversed(range(self.n_steps)):\n",
    "            z = torch.randn(n_samples, self.data_dim) if t > 0 else 0.\n",
    "            model_pred = self.net(torch.cat((x, torch.full((x.shape[0], 1), t, dtype=torch.float32)), dim=1))\n",
    "            x = self.compute_mu_tilde_t(x, t, model_pred) + self.sigmas[t] * z\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b9870d-9a5c-4ff2-bd13-81840715c28e",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ababdb1f-0b27-4d3e-ada2-6316bf60265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_diffusion = JetDiffusion(\n",
    "    data_dim = 4,\n",
    "    n_steps = 100,\n",
    "    n_layers = 3,\n",
    "    hidden_dim = 64,\n",
    "    beta_schedule = \"linear\",\n",
    ")\n",
    "epochs = 50\n",
    "\n",
    "optimizer = torch.optim.Adam(jet_diffusion.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=3e-4, steps_per_epoch=len(train_dataloader), epochs=epochs)\n",
    "\n",
    "losses = np.zeros(epochs)\n",
    "for epoch in range(epochs):\n",
    "    epoch_losses = []\n",
    "    for batch, x in enumerate(train_dataloader):\n",
    "        loss = jet_diffusion.batch_loss(x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "    epoch_loss = np.mean(epoch_losses)\n",
    "    print(f\"Epoch {epoch+1}: loss = {epoch_loss}\")\n",
    "    losses[epoch] = epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b34bc9-d9c8-4058-9be7-b9c13179540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "axs.plot(np.arange(1, epochs+1), losses)\n",
    "axs.set_xlabel(\"loss\")\n",
    "axs.set_ylabel(\"epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0aa4f2-f6ab-41fd-92d0-91948ebfeee8",
   "metadata": {},
   "source": [
    "### Study the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeedfb9b-4cc9-4e3b-a211-0b41a8030927",
   "metadata": {},
   "source": [
    "Let's generate some samples, invert the preprocessing and compute the observables from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabbba89-eefa-4d13-9857-daedc9b0b898",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sample = jet_diffusion.sample(10000).cpu().numpy()\n",
    "sample_pp = invert_preprocessing(sample * train_std + train_mean)\n",
    "gen_event_im, _, gen_pt, _ = get_obs(sample_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01c7aca-6b07-4aad-8293-2493a417e689",
   "metadata": {},
   "source": [
    "We can now plot some observables and compare them to the truth distribution. We start with the energy of the first muon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad38e379-206f-4b04-81c5-1b5df9b0f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "bins = np.linspace(0,200,50)\n",
    "axs.hist(sample_pp[:,0], bins=bins, density=True, histtype=\"step\", label=\"generated\")\n",
    "axs.hist(test_data[:,0], bins=bins, density=True, histtype=\"step\", label=\"truth\")\n",
    "\n",
    "axs.set_xlabel(\"$E_1$ (GeV)\")\n",
    "axs.set_ylabel(\"normalized\")\n",
    "axs.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eca39c-cf1f-449c-b9b4-60bc929ef80d",
   "metadata": {},
   "source": [
    "That looks quite nice! Next, we look at the $p_T$ again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c4535-8110-444d-bd78-17d3c3c25289",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "bins = np.linspace(0, 70, 50)\n",
    "axs.hist(gen_pt, bins=bins, density=True, histtype=\"step\", label=\"generated\")\n",
    "axs.hist(test_muon1_pts, bins=bins, density=True, histtype=\"step\", label=\"truth\")\n",
    "\n",
    "axs.set_xlabel(\"$p_T$ (GeV)\")\n",
    "axs.set_ylabel(\"normalized\")\n",
    "axs.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93320d09-ea07-4f16-81a7-ea58e875ef85",
   "metadata": {},
   "source": [
    "We can see that it is difficult for the network to learn the sharp edge near $m_Z$. Learning such features is a typical difficulty of generative networks. Finally, we can take a look at $m_{\\mu\\mu}$. This observable is challenging to learn because the network needs to extract the correlation between different features correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f38d69e-835a-4828-b303-56da85d596b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "bins = np.linspace(60, 120, 50)\n",
    "axs.hist(gen_event_im, bins=bins, density=True, histtype=\"step\", label=\"generated\")\n",
    "axs.hist(test_event_ims, bins=bins, density=True, histtype=\"step\", label=\"truth\")\n",
    "\n",
    "axs.set_xlabel(\"$m_{\\mu\\mu}$ (GeV)\")\n",
    "axs.set_ylabel(\"normalized\")\n",
    "axs.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f80a8c",
   "metadata": {},
   "source": [
    "#### Exercise 2: Other observables\n",
    "\n",
    "Make histograms of the $\\phi$ and $\\eta$ observables for the first muon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b70c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHI OBSERVABLE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2726edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETA OBSERVABLE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4379adc9",
   "metadata": {},
   "source": [
    "#### Exercise 3: Training length and number of steps\n",
    "\n",
    "The number of epochs and number of time steps often have a large impact on the diffusion model's performance. Run some trainings to see if this is also the case for our example data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9e9c1f-0e57-44df-937d-193bc7de8b5a",
   "metadata": {},
   "source": [
    "### Visualizing the diffusion process\n",
    "\n",
    "To get a better intuition of the diffusion process, we can animate it for an observable of our choice. For that, we need to save the intermediate state of the diffusion at every time step.\n",
    "\n",
    "#### Exercise 4: Intermediate steps\n",
    "\n",
    "Extend the `sample` function such that it returns the intermediate steps of the diffusion. Then you can use the code below to generate an animation of the diffusion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256f6556-73e2-4a67-a4f4-e31219cae125",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sample_xt = jet_diffusion.sample(100000, keep_xt=True).cpu().numpy()\n",
    "sample_ppt = invert_preprocessing(sample_xt * train_std + train_mean)\n",
    "gen_event_im, _, gen_pt, _ = get_obs(sample_ppt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b49cacb-982f-4853-9a48-33068253612b",
   "metadata": {},
   "source": [
    "Now we can use the FuncAnimation class to make an animated plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce03c066-caae-4194-af92-582465889917",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_observable = test_event_ims\n",
    "gen_observable = gen_event_im\n",
    "\n",
    "bins = np.linspace(20,160,50)\n",
    "test_hist, _ = np.histogram(test_observable, bins=bins, density=True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "gen_stairs = ax.stairs(test_hist, edges=bins, label=\"generated\")\n",
    "ax.stairs(test_hist, edges=bins, label=\"truth\")\n",
    "ax.set_xlabel(\"$m_{\\mu\\mu}$ (GeV)\")\n",
    "ax.set_ylabel(\"normalized\")\n",
    "\n",
    "def draw_frame(t):\n",
    "    gen_hist, _ = np.histogram(gen_observable[:, -1 - t], bins=bins, density=True)\n",
    "    gen_stairs.set_data(values=gen_hist, edges=bins)\n",
    "    return (gen_stairs, )\n",
    "\n",
    "anim = animation.FuncAnimation(fig, draw_frame, frames=sample_ppt.shape[1], interval=20, blit=True)\n",
    "plt.close()\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b4bfba",
   "metadata": {},
   "source": [
    "### Bonus exercises\n",
    "\n",
    "#### Exercise A\n",
    "\n",
    "Play around with the hyperparameters of the network. Which ones have a large impact on the performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc5eb18",
   "metadata": {},
   "source": [
    "#### Exercise B\n",
    "\n",
    "As you can see in the animation, not a lot is happening in the early diffusion steps. Try out the cosine schedule that is already implemented in the `JetDiffusion` class and see how that changes the diffusion process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
